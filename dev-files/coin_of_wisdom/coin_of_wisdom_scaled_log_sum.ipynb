{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverend Bayes' legendary coin of wisdom\n",
    "### A parable\n",
    "\n",
    "An eminent Professor of History at your university has recently discovered a relic that was once owned by the venerable Reverend Bayes. The item is the legendary \"coin of wisdom\", said to impart those who possess it with powerful knowledge of statistics. Legend has it that despite appearing to be a fair coin, the coin is biased, with an unequal probability of producing heads or tails. As the Professor of History could barely muster an addition if his life depended on it, he has reached out for collaborative assistance from our group to either confirm or deny the legend.\n",
    "\n",
    "The coin flipping problem is of course a well-known problem from statistics, and when probabilities of heads or tails (or \"probability of success\" if we define success to be either heads or tails) are unequal the process can be modeled using the binomial distribution.\n",
    "\n",
    "You have been tasked with estimating the probability that a flip of the coin produces tails (lets define tails as \"success\"). To do so, you collect a dataset of three sets of coin flips:\n",
    "1. Your professor flips the coin five times, producing 1 tails and 4 heads, before realising he is late to an important meeting and handing the coin to a postdoc in the group.\n",
    "2. The postdoc in the group flips the coin ten times, producing 5 tails and 5 heads, before realising it's time to collect the child from daycare and giving you the coin.\n",
    "3. You flip the coin 100 times, producing 66 tails and 34 heads, before noticing how hungry you are and setting out to have dinner.\n",
    "\n",
    "Because each person may have slight different biases in how they flip coins you decide not to pool all of the results, but instead to treat them as independent estimates under the binomial distribution. And for no particular reason, you decide to take the maximum likelihood computed under the binomial at fixed probabilities of tails/success rather than using analytical estimation or some kind of gradient optimisation.\n",
    "\n",
    "Therefore, you calculate the likelihood of the three observations above when probability of tails ranges from 0.05, 0.1 ... 0.95 in increments of 0.05.\n",
    "\n",
    "What is the maximum likelihood value of the probability of tails, p_t?\n",
    "\n",
    "The likelihood that an arbitrary value p_t produced an observed set of coin flips can be calculated using the probability mass function of the binomial.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import scipy.stats as stats\n",
    "# compute probability mass at each level of fairness for \n",
    "# the professor's coin flips\n",
    "print(\"Professor:\")\n",
    "for i in range(1,20):\n",
    "  print(\"\\tp_t = \"+str(i/20)+\", likelihood=\"+str(stats.binom.pmf(1, 5, i/20)))\n",
    "\n",
    "# now do the same for the postdoc's coin flips\n",
    "print(\"Postdoc:\")\n",
    "for i in range(1,20):\n",
    "  print(\"\\tp_t = \"+str(i/20)+\", likelihood=\"+str(stats.binom.pmf(5, 10, i/20)))\n",
    "\n",
    "# and for the student's coin flips\n",
    "print(\"Student:\")\n",
    "for i in range(1,20):\n",
    "  print(\"\\tp_t = \"+str(i/20)+\", likelihood=\"+str(stats.binom.pmf(66, 100, i/20)))\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor:\n",
      "\tp_t = 0.05, likelihood=0.20362656250000014\n",
      "\tp_t = 0.1, likelihood=0.32804999999999995\n",
      "\tp_t = 0.15, likelihood=0.39150468749999984\n",
      "\tp_t = 0.2, likelihood=0.4095999999999999\n",
      "\tp_t = 0.25, likelihood=0.39550781250000006\n",
      "\tp_t = 0.3, likelihood=0.3601499999999999\n",
      "\tp_t = 0.35, likelihood=0.31238593750000004\n",
      "\tp_t = 0.4, likelihood=0.2592000000000001\n",
      "\tp_t = 0.45, likelihood=0.20588906250000005\n",
      "\tp_t = 0.5, likelihood=0.15624999999999997\n",
      "\tp_t = 0.55, likelihood=0.11276718749999998\n",
      "\tp_t = 0.6, likelihood=0.0768\n",
      "\tp_t = 0.65, likelihood=0.048770312499999996\n",
      "\tp_t = 0.7, likelihood=0.02834999999999999\n",
      "\tp_t = 0.75, likelihood=0.014648437499999998\n",
      "\tp_t = 0.8, likelihood=0.006399999999999999\n",
      "\tp_t = 0.85, likelihood=0.0021515624999999994\n",
      "\tp_t = 0.9, likelihood=0.0004499999999999999\n",
      "\tp_t = 0.95, likelihood=2.968750000000011e-05\n",
      "Postdoc:\n",
      "\tp_t = 0.05, likelihood=6.093524882812493e-05\n",
      "\tp_t = 0.1, likelihood=0.0014880347999999995\n",
      "\tp_t = 0.15, likelihood=0.008490855786328114\n",
      "\tp_t = 0.2, likelihood=0.026424115199999983\n",
      "\tp_t = 0.25, likelihood=0.058399200439453146\n",
      "\tp_t = 0.3, likelihood=0.10291934519999989\n",
      "\tp_t = 0.35, likelihood=0.15357041070820307\n",
      "\tp_t = 0.4, likelihood=0.20065812479999992\n",
      "\tp_t = 0.45, likelihood=0.23403270759257816\n",
      "\tp_t = 0.5, likelihood=0.24609375000000003\n",
      "\tp_t = 0.55, likelihood=0.23403270759257822\n",
      "\tp_t = 0.6, likelihood=0.20065812479999992\n",
      "\tp_t = 0.65, likelihood=0.15357041070820307\n",
      "\tp_t = 0.7, likelihood=0.10291934520000007\n",
      "\tp_t = 0.75, likelihood=0.058399200439453146\n",
      "\tp_t = 0.8, likelihood=0.026424115199999956\n",
      "\tp_t = 0.85, likelihood=0.008490855786328126\n",
      "\tp_t = 0.9, likelihood=0.0014880347999999982\n",
      "\tp_t = 0.95, likelihood=6.0935248828125204e-05\n",
      "Student:\n",
      "\tp_t = 0.05, likelihood=1.3759027136263122e-60\n",
      "\tp_t = 0.1, likelihood=1.615140034501527e-41\n",
      "\tp_t = 0.15, likelihood=9.687939837461297e-31\n",
      "\tp_t = 0.2, likelihood=2.172721854371014e-23\n",
      "\tp_t = 0.25, likelihood=6.026822332210419e-18\n",
      "\tp_t = 0.3, likelihood=9.711829138028317e-14\n",
      "\tp_t = 0.35, likelihood=2.0488488209936218e-10\n",
      "\tp_t = 0.4, likelihood=9.058719287045428e-08\n",
      "\tp_t = 0.45, likelihood=1.1176921820504125e-05\n",
      "\tp_t = 0.5, likelihood=0.0004581052772872398\n",
      "\tp_t = 0.55, likelihood=0.006872707109016188\n",
      "\tp_t = 0.6, likelihood=0.03908292791814707\n",
      "\tp_t = 0.65, likelihood=0.08214098520943502\n",
      "\tp_t = 0.7, likelihood=0.05788395038595346\n",
      "\tp_t = 0.75, likelihood=0.0111678234562091\n",
      "\tp_t = 0.8, likelihood=0.00040079643990937475\n",
      "\tp_t = 0.85, likelihood=1.2379889441291375e-06\n",
      "\tp_t = 0.9, likelihood=5.545880203974579e-11\n",
      "\tp_t = 0.95, likelihood=1.144729718323783e-19\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining information from the separate trials\n",
    "\n",
    "We have now obtained the likelihoods of different p_t for each of the three independent coin flip trials. How do we combine them? One approach would be to sum the (log) likelihoods across trials and take the maximum. Does it produce the correct result? Which trial contributes the most to the estimate of p_t?\n",
    "\n",
    "\n",
    "\n",
    "## A further correspondence from the historian\n",
    "\n",
    "The historian has come into contact with a colleague in Europe who is in possession of a letter written by the reverend that describes the manufacture of the coin. In it, the reverend indicates the shape and weight of the coin were designed to produce tails 60% of the time. How does that compare to your estimate? How does it compare to the student's estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.016622353441251 -3.7226914442392793 -30.894898798704475\n",
      "0.810333705768002 0.09764143825477582\n"
     ]
    }
   ],
   "source": [
    "# scale factor\n",
    "import numpy as np\n",
    "prof_lk = np.log([stats.binom.pmf(1, 5, i/20) for i in range(1,20)])\n",
    "postdoc_lk = np.log([stats.binom.pmf(5, 10, i/20) for i in range(1,20)])\n",
    "student_lk = np.log([stats.binom.pmf(66, 100, i/20) for i in range(1,20)])\n",
    "avg_prof_lk = np.average(prof_lk)\n",
    "avg_postdoc_lk = np.average(postdoc_lk)\n",
    "avg_student_lk = np.average(student_lk)\n",
    "print(avg_prof_lk,avg_postdoc_lk,avg_student_lk)\n",
    "print(avg_prof_lk/avg_postdoc_lk, avg_prof_lk/avg_student_lk) # scaling factor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0.17434374999999996"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.010422094818650304*16.728282848474226"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "postdoc_lk_scaled = np.array(postdoc_lk) * (avg_prof_lk/avg_postdoc_lk)\n",
    "student_lk_scaled = np.array(student_lk) * (avg_prof_lk/avg_student_lk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.05   -7.864855\n0.10   -5.275515\n0.15   -3.864291\n0.20   -2.944330\n0.25   -2.301715\n0.30   -1.842545\n0.35   -1.518238\n0.40   -1.301520\n0.45   -1.176843\n0.50   -1.136122\n0.55   -1.176843\n0.60   -1.301520\n0.65   -1.518238\n0.70   -1.842545\n0.75   -2.301715\n0.80   -2.944330\n0.85   -3.864291\n0.90   -5.275515\n0.95   -7.864855\ndtype: float64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "probabilities = [i/20 for i in range(1,20)]\n",
    "prof_vals = pd.Series(prof_lk,probabilities)\n",
    "postdoc_vals = pd.Series(postdoc_lk_scaled,probabilities)\n",
    "student_vals = pd.Series(student_lk_scaled,probabilities)\n",
    "postdoc_vals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "              0         1          2\n0.05  -1.591468 -7.864855 -13.458505\n0.10  -1.114589 -5.275515  -9.171125\n0.15  -0.937758 -3.864291  -6.747927\n0.20  -0.892574 -2.944330  -5.095270\n0.25  -0.927585 -2.301715  -3.871513\n0.30  -1.021235 -1.842545  -2.925615\n0.35  -1.163516 -1.518238  -2.178241\n0.40  -1.350155 -1.301520  -1.583447\n0.45  -1.580418 -1.176843  -1.113274\n0.50  -1.856298 -1.136122  -0.750708\n0.55  -2.182430 -1.176843  -0.486274\n0.60  -2.566551 -1.301520  -0.316560\n0.65  -3.020634 -1.518238  -0.244037\n0.70  -3.563128 -1.842545  -0.278211\n0.75  -4.223422 -2.301715  -0.438871\n0.80  -5.051457 -2.944330  -0.763757\n0.85  -6.141561 -3.864291  -1.328121\n0.90  -7.706263 -5.275515  -2.305840\n0.95 -10.424784 -7.864855  -4.258529",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.05</th>\n      <td>-1.591468</td>\n      <td>-7.864855</td>\n      <td>-13.458505</td>\n    </tr>\n    <tr>\n      <th>0.10</th>\n      <td>-1.114589</td>\n      <td>-5.275515</td>\n      <td>-9.171125</td>\n    </tr>\n    <tr>\n      <th>0.15</th>\n      <td>-0.937758</td>\n      <td>-3.864291</td>\n      <td>-6.747927</td>\n    </tr>\n    <tr>\n      <th>0.20</th>\n      <td>-0.892574</td>\n      <td>-2.944330</td>\n      <td>-5.095270</td>\n    </tr>\n    <tr>\n      <th>0.25</th>\n      <td>-0.927585</td>\n      <td>-2.301715</td>\n      <td>-3.871513</td>\n    </tr>\n    <tr>\n      <th>0.30</th>\n      <td>-1.021235</td>\n      <td>-1.842545</td>\n      <td>-2.925615</td>\n    </tr>\n    <tr>\n      <th>0.35</th>\n      <td>-1.163516</td>\n      <td>-1.518238</td>\n      <td>-2.178241</td>\n    </tr>\n    <tr>\n      <th>0.40</th>\n      <td>-1.350155</td>\n      <td>-1.301520</td>\n      <td>-1.583447</td>\n    </tr>\n    <tr>\n      <th>0.45</th>\n      <td>-1.580418</td>\n      <td>-1.176843</td>\n      <td>-1.113274</td>\n    </tr>\n    <tr>\n      <th>0.50</th>\n      <td>-1.856298</td>\n      <td>-1.136122</td>\n      <td>-0.750708</td>\n    </tr>\n    <tr>\n      <th>0.55</th>\n      <td>-2.182430</td>\n      <td>-1.176843</td>\n      <td>-0.486274</td>\n    </tr>\n    <tr>\n      <th>0.60</th>\n      <td>-2.566551</td>\n      <td>-1.301520</td>\n      <td>-0.316560</td>\n    </tr>\n    <tr>\n      <th>0.65</th>\n      <td>-3.020634</td>\n      <td>-1.518238</td>\n      <td>-0.244037</td>\n    </tr>\n    <tr>\n      <th>0.70</th>\n      <td>-3.563128</td>\n      <td>-1.842545</td>\n      <td>-0.278211</td>\n    </tr>\n    <tr>\n      <th>0.75</th>\n      <td>-4.223422</td>\n      <td>-2.301715</td>\n      <td>-0.438871</td>\n    </tr>\n    <tr>\n      <th>0.80</th>\n      <td>-5.051457</td>\n      <td>-2.944330</td>\n      <td>-0.763757</td>\n    </tr>\n    <tr>\n      <th>0.85</th>\n      <td>-6.141561</td>\n      <td>-3.864291</td>\n      <td>-1.328121</td>\n    </tr>\n    <tr>\n      <th>0.90</th>\n      <td>-7.706263</td>\n      <td>-5.275515</td>\n      <td>-2.305840</td>\n    </tr>\n    <tr>\n      <th>0.95</th>\n      <td>-10.424784</td>\n      <td>-7.864855</td>\n      <td>-4.258529</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_df = pd.concat([prof_vals,postdoc_vals,student_vals], axis=1)\n",
    "collected_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0.05   -22.914827\n0.10   -15.561229\n0.15   -11.549976\n0.20    -8.932174\n0.25    -7.100813\n0.30    -5.789395\n0.35    -4.859995\n0.40    -4.235122\n0.45    -3.870535\n0.50    -3.743128\n0.55    -3.845547\n0.60    -4.184631\n0.65    -4.782909\n0.70    -5.683884\n0.75    -6.964007\n0.80    -8.759544\n0.85   -11.333973\n0.90   -15.287617\n0.95   -22.548168\ndtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_df_sum = collected_df.sum(axis=1)\n",
    "collected_df_sum"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}